{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries for classfication\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression #Regression\n",
    "from sklearn.linear_model import LinearRegression #Regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score #Regression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score,f1_score,confusion_matrix,PrecisionRecallDisplay\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV,cross_val_score,StratifiedKFold, LeaveOneOut\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "#for precision recall curve\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve,matthews_corrcoef\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from collections import Counter\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from scipy.stats import ttest_rel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREVISUALIZACION DE LOS DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_correlation_matrix(df):\n",
    "    corr = df.corr()\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr, annot=True, fmt=\".2f\", square=True, cbar=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATASET LOADING\n",
    "#ESTE ES EL DATASET SIN MODIFICAR\n",
    "df = pd.read_csv(\"B.HEALTH_classification.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRAFICA DEL DATASET\n",
    "#GRAFICAMOS CANTIDAD DE DATOS POR CLASE\n",
    "#0 es buena calidad 2 es mala calidad\n",
    "sns.countplot(x='label', data=df)\n",
    "plt.show()\n",
    "print(df[\"label\"].value_counts())\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MOSTRAR MATRIZ DE CORRELACION\n",
    "base_df = df.drop(columns=[\"subject\",\"day\"])\n",
    "columns = [] #si queremos ver la matriz de correlacion sin alguna columna agregarlo aqui\n",
    "show_correlation_matrix(base_df.drop(columns=columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " PREPARACION DE LOS DATOS PARA EL ENTRENAMIENTO Y VISUALIZACION DE CARACTERISTICAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(X, how=\"minmax\"):\n",
    "    if how == \"minmax\":\n",
    "        scaler = MinMaxScaler()\n",
    "    elif how == \"standard\":\n",
    "        scaler = StandardScaler()\n",
    "    return scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_tsne(X, n_components=2, pca=False):\n",
    "    if pca:\n",
    "        reduccion = TSNE(n_components, init='pca')\n",
    "    else:\n",
    "        reduccion = TSNE(n_components)\n",
    "    x_new = reduccion.fit_transform(X)\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_TSNE(x,y): #plot with TSNE and PCA to reduce dimensions\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "    \n",
    "    x_new = apply_tsne(x, n_components=2, pca=True)\n",
    "    tmp_df = pd.DataFrame(np.column_stack([x_new, y]))\n",
    "    tmp_df.columns = [\"x1\", \"x2\", \"Y\"]\n",
    "    \n",
    "    sns.scatterplot(x=\"x1\", y=\"x2\", hue=\"Y\", data=tmp_df, ax=ax)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate test and train data\n",
    "def split_data(X, y, test_size=0.3):\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVEMOS COLUMNAS QUE NO QUEREMOS Y APLICAMOS SMOTE DE SER NECESARIO\n",
    "remove_columns = [\"subject\",\"day\", \"label\"] #remover columnas que no queremos para las variables\n",
    "use_smote = True\n",
    "\n",
    "X, Y = df.drop(columns=remove_columns), df[\"label\"]\n",
    "\n",
    "if use_smote:\n",
    "    smote = SMOTE(sampling_strategy='minority')\n",
    "    X, Y = smote.fit_resample(df.drop(columns=remove_columns), df[\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NORMALIZAMOS LOS DATOS\n",
    "X = normalize_data(X, how=\"minmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEPARAMOS LOS DATOS DE ENTRENAMIENTO Y PRUEBA\n",
    "#SI NO ESTA USANDO SMOTE PROCURAR 50% DE LA\n",
    "X_train, X_test, y_train, y_test = split_data(X, Y, test_size=0.2)\n",
    "print(y_train.value_counts())\n",
    "sns.countplot(x=y_train)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOTTING DATA\n",
    "plot_data_TSNE(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENTRENAMIENTO Y TUNNING DE MODELOS PARA CLASIFICACION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(model_name=\"DecisionTree\"): \n",
    "    if model_name == \"DecisionTree\":\n",
    "        return DecisionTreeClassifier()\n",
    "    elif model_name == \"SVM\":\n",
    "        return SVC()\n",
    "    elif model_name == \"RandomForest\":\n",
    "        return RandomForestClassifier()\n",
    "    elif model_name == \"XGBoost\":\n",
    "        return XGBClassifier()\n",
    "    elif model_name == \"MLP\":\n",
    "        return MLPClassifier()\n",
    "    else:\n",
    "        print(\"Invalid model name\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparing each model accuracy\n",
    "def compare_models_accuracy(X_train, y_train, X_test, y_test):\n",
    "    models = [\"DecisionTree\", \"SVM\", \"RandomForest\"]\n",
    "    accuracies = []\n",
    "    for model_name in models:\n",
    "        model = select_model(model_name)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "    return list(zip(models, accuracies))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models_accuracy(X_train, y_train, X_test, y_test):\n",
    "    models = [\"DecisionTree\", \"SVM\", \"RandomForest\", \"XGBoost\"]\n",
    "    accuracies = []\n",
    "    for model_name in models:\n",
    "        model = select_model(model_name)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "    return list(zip(models, accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting comparison of models with no hyperparameter tuning\n",
    "comparison_list = compare_models_accuracy(X_train, y_train, X_test, y_test)\n",
    "print(comparison_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_tunning(model,parameters, method=\"grid\", scoring_metric = \"accuracy\", cv = 5):\n",
    "    if method == \"grid\":\n",
    "        clf = GridSearchCV(model, param_grid=parameters, scoring=scoring_metric, cv=cv, n_jobs=-1)  \n",
    "    # clf_svc = RandomizedSearchCV(clf_svc, param_distributions=parameters, scoring=\"accuracy\", cv=5, n_jobs=-1, n_iter=15)\n",
    "    else:\n",
    "        clf = RandomizedSearchCV(model, param_distributions=parameters, scoring=scoring_metric, cv=cv, n_jobs=-1, n_iter=15)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"-----------------\")\n",
    "    print(\"Model\", model.__class__.__name__)\n",
    "    print(\"Best score : \", clf.best_score_)\n",
    "    print(\"Best Parameters : \", clf.best_params_)\n",
    "    print(\"-----------------\")\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Model\", model.__class__.__name__)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"F1 Score: \", f1)\n",
    "    print(\"Confusion Matrix: \")\n",
    "    print(confusion)\n",
    "    return accuracy, f1, confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = {\n",
    "#    \"n_estimators\":[i for i in range(1,200)],                   \n",
    "#    \"learning_rate\":[0.1*i for i in range(1,10)],\n",
    "#    \"max_depth\":[i for i in range(1,20)],                                            \n",
    "# }\n",
    "# useGridSearch = False\n",
    "# parameter_tunning(select_model(\"XGBoost\"), parameters, useGridSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DecisionTree parameters\n",
    "# weights = np.linspace(0.0,0.99,20)\n",
    "# print(weights)\n",
    "# parameters = {\n",
    "#     \"criterion\": [\"gini\", \"entropy\",\"log_loss\"],\n",
    "#     \"max_depth\": [i if i!=0 else None for i in range(1, 10)],\n",
    "#     \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "#     'class_weight': [{0:x, 1:1.0-x , 2:1.0 - x} for x in weights]\n",
    "# }\n",
    "# model = DecisionTreeClassifier()\n",
    "# parameter_tunning(model, parameters, scoring_metric=\"f1_weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision_tree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=9, max_features=\"sqrt\")\n",
    "# decision_tree.fit(X_train, y_train)\n",
    "# y_pred = decision_tree.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy DecisionTree: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm = SVC()\n",
    "# #weights = np.linspace(0.0,0.99,200)\n",
    "# parameters = {\n",
    "#     \"C\": [0.1, 1, 10, 100],\n",
    "#     \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"], \n",
    "#     \"gamma\": [0.01*i for i in range(1, 100)],\n",
    "#     #'class_weight': [{0:x, 1:1.0-x,2:1.0 - x} for x in weights]\n",
    "# }\n",
    "# parameter_tunning(svm, parameters,scoring_metric=\"f1_weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm = SVC(C=10, degree=2, gamma=0.44, kernel=\"rbf\")\n",
    "# clf = svm.fit(X_train, y_train)\n",
    "# y_pred = svm.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy SVM: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    \"\"\"Create a mesh of points to plot in\"\"\"\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\"\"\"\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "#APPLYTING TSNE TO VISUALIZE DATA WITH SVC\n",
    "# plot_data_TSNE(X, Y)\n",
    "# plt.show()\n",
    "# X_tsne = apply_tsne(X, n_components=2, pca=True)\n",
    "# X_train_tsne, X_test_tsne, y_train_tsne, y_test_tsne = split_data(X_tsne, Y)\n",
    "# clf = svm.fit(X_train_tsne, y_train_tsne)\n",
    "\n",
    "#  # Create a meshgrid for plotting\n",
    "# X0, X1 = X_tsne[:, 0], X_tsne[:, 1]\n",
    "# xx, yy = make_meshgrid(X0, X1)\n",
    "\n",
    "# # Plot the decision boundaries\n",
    "# fig, ax = plt.subplots()\n",
    "# plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "# ax.scatter(X0, X1, c=Y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "# ax.set_xlabel('TSNE Component 1')\n",
    "# ax.set_ylabel('TSNE Component 2')\n",
    "# ax.set_title('SVM Decision Boundary')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_f= RandomForestClassifier()\n",
    "\n",
    "# parameters = {\n",
    "#     \"n_estimators\": [50, 100, 200, 300],\n",
    "#     \"criterion\": [\"gini\", \"entropy\",\"log_loss\"],\n",
    "#     \"max_depth\": [i if i!=0 else None for i in range(1, 10)],\n",
    "#     \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "#     'class_weight': [{0:x, 1:1.0-x , 2:1.0 - x} for x in weights]\n",
    "# }\n",
    "# parameter_tunning(random_f, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_f= RandomForestClassifier(criterion=\"entropy\", max_depth=5, max_features=\"log2\", n_estimators=50)\n",
    "# random_f.fit(X_train, y_train)\n",
    "# y_pred = random_f.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy RandomForest: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost = XGBClassifier()\n",
    "# parameters = {\n",
    "#    \"n_estimators\":[i for i in range(1,200)],                   \n",
    "#    \"learning_rate\":[0.1*i for i in range(1,10)],\n",
    "#    \"max_depth\":[i for i in range(1,20)],\n",
    "#    \"max_features\":[i for i in range(1,20)],\n",
    "#    'class_weight': [{0:x, 1:1.0-x , 2:1.0 - x} for x in weights]\n",
    "                                                  \n",
    "# }\n",
    "# parameter_tunning(xgboost, parameters, method=\"random\", scoring_metric=\"f1_weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp_model = MLPClassifier()\n",
    "# parameters = {\n",
    "#     \"hidden_layer_sizes\": [(100,), (200,), (300,)],\n",
    "#     \"activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
    "#     \"solver\": [\"lbfgs\", \"sgd\", \"adam\"],\n",
    "#     \"alpha\": [0.0001, 0.001, 0.01, 0.1],\n",
    "#     \"learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"]\n",
    "# }\n",
    "# parameter_tunning(mlp_model, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp_model = MLPClassifier(activation=\"relu\", alpha=0.1, hidden_layer_sizes=(100,), learning_rate=\"invscaling\", solver=\"adam\")\n",
    "# mlp_model.fit(X_train, y_train)\n",
    "# y_pred = mlp_model.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy MLP: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LISTA DE MODELOS Y PARAMETROS PARA TUNEAR\n",
    "models = [DecisionTreeClassifier(),SVC(),RandomForestClassifier(),XGBClassifier()]\n",
    "weights = np.linspace(0.0,0.9,10) #lista de pesos para el entrenamiento con datos desbalanceados\n",
    "parameters = [\n",
    "    {\n",
    "        \"criterion\": [\"gini\", \"entropy\",\"log_loss\"],\n",
    "        \"max_depth\": [i if i!=0 else None for i in range(0, 10)],\n",
    "        \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "        'class_weight': [{2:x} for x in weights]\n",
    "    },\n",
    "    {\n",
    "        \"C\": [0.1, 1, 10, 100],\n",
    "        \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"], \n",
    "        \"gamma\": [0.1*i for i in range(1, 100)],\n",
    "        'class_weight': [{2:x} for x in weights]\n",
    "    },\n",
    "    {\n",
    "        \"n_estimators\": [50, 100, 200, 300],\n",
    "        \"criterion\": [\"gini\", \"entropy\",\"log_loss\"],\n",
    "        \"max_depth\": [i if i!=0 else None for i in range(0, 10)],\n",
    "        \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "        'class_weight': [{2:x} for x in weights]\n",
    "    },\n",
    "    {\n",
    "        \"n_estimators\":[i for i in range(1,200)],                   \n",
    "        \"learning_rate\":[0.1*i for i in range(1,10)],\n",
    "        \"max_depth\":[i for i in range(1,20)],\n",
    "        \"max_features\":[i for i in range(1,20)],\n",
    "        'class_weight': [{2:x} for x in weights]\n",
    "    },\n",
    "]\n",
    "\n",
    "#TUNEO DE HIPERPARAMETROS A CADA MODELO Y VALIDACION CRUZADA PARA CADA MODELO\n",
    "#UTILIZAMOS TUNEO DE HIPERPARAMETROS CON VALIDACION CRUZADA PARA UN CRITERIO PRINCIPAL,\n",
    "#DESPUES VERIFICAMOS EL MODELO CON CROSS VALI\n",
    "best_models_with_scores = []\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "for i in range(len(models)):\n",
    "    model = models[i]\n",
    "    params = parameters[i]\n",
    "    clf = parameter_tunning(model, params, method=\"random\", scoring_metric=\"f1_weighted\", cv=cv) #this shall return the best model with parameters\n",
    "\n",
    "    best_models_with_scores.append((clf.best_estimator_, clf.best_score_, clf.best_params_))\n",
    "\n",
    "best_models_with_scores    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANALISIS Y COMPARACION DE MODELOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decimal_to_time(decimal_time): ##calculo para pasar tiempo decimal a horas minutos y segundos\n",
    "    \"\"\"\n",
    "    Convert a decimal time representation to HH:MM:SS format.\n",
    "    \n",
    "    Parameters:\n",
    "    decimal_time (float): The decimal representation of time.\n",
    "    \n",
    "    Returns:\n",
    "    str: The time in HH:MM:SS format.\n",
    "    \"\"\"\n",
    "    total_seconds = int(decimal_time * 24 * 3600)\n",
    "    hours = total_seconds // 3600\n",
    "    minutes = (total_seconds % 3600) // 60\n",
    "    seconds = total_seconds % 60\n",
    "    return f\"{hours:02}:{minutes:02}:{seconds:02}\"\n",
    "\n",
    "\n",
    "print(decimal_to_time(0.457638889)) \n",
    "print(decimal_to_time(0.707638889)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_precision_recall_curve(y_test, y_score, n_classes=3):\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    for i in range(n_classes):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_test[:, i],\n",
    "                                                            y_score[:, i])\n",
    "        plt.plot(recall[i], precision[i], lw=2, label='class {}'.format(i))\n",
    "        \n",
    "    plt.xlabel(\"recall\")\n",
    "    plt.ylabel(\"precision\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"precision vs. recall curve\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #classifiers with best hyperparameters\n",
    "if use_smote:\n",
    "# #SMOTE\n",
    "    decision_tree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=5, max_features=\"log2\")\n",
    "    xgbModel = XGBClassifier(learning_rate=0.2, max_depth=19, n_estimators=154)\n",
    "    svm = SVC(C=100, degree=2, gamma=0.09, kernel=\"rbf\",probability=True)\n",
    "    random_f= RandomForestClassifier(criterion=\"entropy\", max_depth=6, max_features=\"sqrt\", n_estimators=50)\n",
    "    # mlp_model = MLPClassifier(activation=\"relu\", alpha=0.01, hidden_layer_sizes=(200,), learning_rate=\"invscaling\", solver=\"adam\")\n",
    "else:\n",
    "# # #WEIGHTED\n",
    "    decision_tree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=None, max_features=\"sqrt\",class_weight={2: np.float64(0.7)})\n",
    "    xgbModel = XGBClassifier(learning_rate=0.4, max_depth=2,n_estimators=38)\n",
    "    #xgbModel = XGBClassifier(learning_rate=0.4, max_depth=2, max_features=11, n_estimators=38,class_weight={2: np.float64(0.7)})\n",
    "    svm = SVC(C=0.1, gamma=1, kernel=\"poly\",probability=True,class_weight={2:0.9}) \n",
    "    random_f= RandomForestClassifier(criterion=\"log_loss\", max_depth=7, max_features=\"sqrt\", n_estimators=50 , class_weight= {2: np.float64(0.9)} )\n",
    "    # mlp_model = MLPClassifier(activation=\"relu\", alpha=0.01, hidden_layer_sizes=(200,), learning_rate=\"invscaling\", solver=\"adam\")\n",
    "decision_tree.fit(X_train, y_train)\n",
    "svm.fit(X_train, y_train)\n",
    "random_f.fit(X_train, y_train)\n",
    "xgbModel.fit(X_train, y_train)\n",
    "# mlp_model.fit(X_train, y_train)\n",
    "evaluate_model(decision_tree, X_test, y_test)\n",
    "evaluate_model(svm, X_test, y_test)\n",
    "evaluate_model(random_f, X_test, y_test)\n",
    "evaluate_model(xgbModel, X_test, y_test)\n",
    "# evaluate_model(mlp_model, X_test, y_test)\n",
    "\n",
    "models = [decision_tree, svm, random_f, xgbModel]\n",
    "#CROSS VALIDATION WITH TEST DATA\n",
    "cross_val_scores = []\n",
    "# cv = StratifiedKFold(n_splits=5)\n",
    "cv = StratifiedKFold(n_splits=3)\n",
    "for model in models:\n",
    "    if use_smote:\n",
    "        scores = cross_val_score(model, X_test, y_test, cv=cv, scoring=\"accuracy\")\n",
    "    else:\n",
    "        scores = cross_val_score(model, X_test, y_test, cv=cv, scoring=\"f1_weighted\")\n",
    "    cross_val_scores.append((model.__class__.__name__,scores.mean(), scores.std()))\n",
    "\n",
    "print(\"Cross Validation Scores\")\n",
    "for model, mean, std in cross_val_scores:\n",
    "    print(f\"Model: {model}, Mean: {mean}, Std: {std}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WEIGHING CLASSES\n",
    "# svm = SVC(probability=True)\n",
    "\n",
    "# weights = np.linspace(0.0,0.99,200)\n",
    "# param_grid = {'class_weight': [{0:x, 1:1.0-x} for x in weights]}\n",
    "# #Fitting grid search to the train data with 5 folds\n",
    "# gridsearch_csv_results = parameter_tunning(svm, param_grid, method=\"grid\")\n",
    "\n",
    "# #Ploting the score for different values of weight\n",
    "# sns.set_style('whitegrid')\n",
    "# plt.figure(figsize=(12,8))\n",
    "# weigh_data = pd.DataFrame({ 'score': gridsearch_csv_results['mean_test_score'], 'weight': (1- weights)})\n",
    "# sns.lineplot(data=weigh_data, y='score', x='weight')\n",
    "# plt.xlabel('Weight for class 1')\n",
    "# plt.ylabel('F1 score')\n",
    "# plt.xticks([round(i/10,1) for i in range(0,11,1)])\n",
    "# plt.title('Scoring for different class weights', fontsize=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing model with best hyperparameters and weights or smote\n",
    "# model = SVC(C=100, gamma=0.03, kernel=\"rbf\",probability=True,class_weight={0: 0.5223618090452261, 1:0.4776381909547739, 2:0.4776381909547739}) #model without smote and applying weights\n",
    "# #model = SVC(C=100, degree=2, gamma=0.09, kernel=\"rbf\",probability=True) #model with smote\n",
    "# evaluate_model(model,X_test, y_test)\n",
    "\n",
    "# n_classes = 3\n",
    "# Y_pr = label_binarize(Y, classes=[0, 1, 2])#esto es necesario para la curva de precision recall\n",
    "# for model in best_models_with_scores:\n",
    "#     y_score = model[0].predict_proba(X_test)\n",
    "#     graph_precision_recall_curve(y_test, y_score, n_classes)\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "\n",
    "def plot_roc_and_pr_curves(y_test, y_score, classes):\n",
    "    n_classes = y_test.shape[1]\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.figure()\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label='micro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"micro\"]),\n",
    "             color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        plt.plot(fpr[i], tpr[i], lw=2,\n",
    "                 label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                       ''.format(classes[i], roc_auc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    # Compute Precision-Recall curve and average precision for each class\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    average_precision = dict()\n",
    "    for i in range(n_classes):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_test[:, i], y_score[:, i])\n",
    "        average_precision[i] = average_precision_score(y_test[:, i], y_score[:, i])\n",
    "\n",
    "    # Compute micro-average Precision-Recall curve and average precision\n",
    "    precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_test.ravel(), y_score.ravel())\n",
    "    average_precision[\"micro\"] = average_precision_score(y_test, y_score, average=\"micro\")\n",
    "\n",
    "    # Plot Precision-Recall curve\n",
    "    plt.figure()\n",
    "    plt.plot(recall[\"micro\"], precision[\"micro\"],\n",
    "             label='micro-average Precision-Recall curve (area = {0:0.2f})'\n",
    "                   ''.format(average_precision[\"micro\"]),\n",
    "             color='gold', linestyle=':', linewidth=4)\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        plt.plot(recall[i], precision[i], lw=2,\n",
    "                 label='Precision-Recall curve of class {0} (area = {1:0.2f})'\n",
    "                       ''.format(classes[i], average_precision[i]))\n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming y_test and y_score are your test labels and predicted scores respectively\n",
    "# and classes is a list of class names or labels\n",
    "y_score = svm.predict_proba(X_test)\n",
    "Y_test_binarize = label_binarize(y_test, classes=[0, 1, 2])\n",
    "classes = [0, 1, 2]\n",
    "plot_roc_and_pr_curves(Y_test_binarize, y_score, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcnemar_test_between_models(models, X_test, y_test, alpha=0.05):\n",
    "    n_models = len(models)\n",
    "    wins = np.zeros(n_models)\n",
    "    \n",
    "    # Convert y_test to a numpy array\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    # Get predictions for each model\n",
    "    predictions = [model.predict(X_test) for model in models]\n",
    "    \n",
    "    # Perform pairwise McNemar tests\n",
    "    for i in range(n_models):\n",
    "        for j in range(i + 1, n_models):\n",
    "            # Create contingency table\n",
    "            contingency_table = np.zeros((2, 2))\n",
    "            for k in range(len(y_test)):\n",
    "                if predictions[i][k] == y_test[k] and predictions[j][k] == y_test[k]:\n",
    "                    contingency_table[0, 0] += 1\n",
    "                elif predictions[i][k] == y_test[k] and predictions[j][k] != y_test[k]:\n",
    "                    contingency_table[0, 1] += 1\n",
    "                elif predictions[i][k] != y_test[k] and predictions[j][k] == y_test[k]:\n",
    "                    contingency_table[1, 0] += 1\n",
    "                else:\n",
    "                    contingency_table[1, 1] += 1\n",
    "            \n",
    "            # Perform McNemar test\n",
    "            result = mcnemar(contingency_table, exact=True)\n",
    "            p_value = result.pvalue\n",
    "            \n",
    "            # Print contingency table and p-value\n",
    "            print(f\"Comparison between model {i} and model {j}:\")\n",
    "            print(\"Contingency table:\")\n",
    "            print(contingency_table)\n",
    "            print(f\"P-value: {p_value}\\n\")\n",
    "            \n",
    "            # Update wins based on p-value\n",
    "            if p_value < alpha:\n",
    "                if contingency_table[0, 1] > contingency_table[1, 0]:\n",
    "                    wins[i] += 1\n",
    "                else:\n",
    "                    wins[j] += 1\n",
    "    \n",
    "    # Determine the best model\n",
    "    best_model_index = np.argmax(wins)\n",
    "    best_model = models[best_model_index]\n",
    "    \n",
    "    return best_model, wins\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have trained models: model1, model2, model3\n",
    "# and you have test data: X_test, y_test\n",
    "\n",
    "models = [xgbModel, decision_tree, random_f, svm]\n",
    "best_model, wins = mcnemar_test_between_models(models, X_test, y_test, alpha=0.05)\n",
    "\n",
    "print(\"Best model:\", best_model)\n",
    "print(\"Wins:\", wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_5x2cv_paired_t_test(model1, model2, X, y):\n",
    "    \"\"\"\n",
    "    Perform 5x2 cross-validated paired t-test to compare two models.\n",
    "    \n",
    "    Parameters:\n",
    "    model1: First model to compare\n",
    "    model2: Second model to compare\n",
    "    X: Feature matrix\n",
    "    y: Target vector\n",
    "    \n",
    "    Returns:\n",
    "    t_stat: t-statistic\n",
    "    p_value: p-value\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "    scores1 = []\n",
    "    scores2 = []\n",
    "    y = np.array(y)\n",
    "    \n",
    "    for i in range(5):\n",
    "        for train_index, test_index in skf.split(X, y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            \n",
    "            model1.fit(X_train, y_train)\n",
    "            model2.fit(X_train, y_train)\n",
    "            \n",
    "            # evaluate_model(model1, X_test, y_test)\n",
    "            # evaluate_model(model2, X_test, y_test)\n",
    "            \n",
    "            score1 = model1.score(X_test, y_test)\n",
    "            score2 = model2.score(X_test, y_test)\n",
    "            \n",
    "            scores1.append(score1)\n",
    "            scores2.append(score2)\n",
    "    \n",
    "    scores1 = np.array(scores1)\n",
    "    scores2 = np.array(scores2)\n",
    "    \n",
    "    # Compute the differences in scores\n",
    "    differences = scores1 - scores2\n",
    "    \n",
    "    # Compute the t-statistic and p-value\n",
    "    t_stat, p_value = ttest_rel(scores1, scores2)\n",
    "    \n",
    "    return t_stat, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_stat, p_value = cross_val_5x2cv_paired_t_test(xgbModel, random_f, X_test, y_test)\n",
    "print(\"t-statistic:\", t_stat)\n",
    "print(\"p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matthews_corrcoef(y_test, xgbModel.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matthews_corrcoef(y_test, svm.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matthews_corrcoef(y_test, random_f.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matthews_corrcoef(y_test, decision_tree.predict(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
