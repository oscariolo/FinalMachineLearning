{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries for classfication\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression #Regression\n",
    "from sklearn.linear_model import LinearRegression #Regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score #Regression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score,f1_score,confusion_matrix,PrecisionRecallDisplay\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "#for precision recall curve\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREVISUALIZACION DE LOS DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_correlation_matrix(df):\n",
    "    corr = df.corr()\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr, annot=True, fmt=\".2f\", square=True, cbar=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATASET LOADING\n",
    "#ESTE ES EL DATASET SIN MODIFICAR\n",
    "df = pd.read_csv(\"B.HEALTH_classification.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRAFICA DEL DATASET\n",
    "#GRAFICAMOS CANTIDAD DE DATOS POR CLASE\n",
    "#0 es buena calidad 2 es mala calidad\n",
    "sns.countplot(x='label', data=df)\n",
    "plt.show()\n",
    "print(df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MOSTRAR MATRIZ DE CORRELACION\n",
    "base_df = df.drop(columns=[\"subject\",\"day\"])\n",
    "columns = [] #si queremos ver la matriz de correlacion sin alguna columna agregarlo aqui\n",
    "show_correlation_matrix(base_df.drop(columns=columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " PREPARACION DE LOS DATOS PARA EL ENTRENAMIENTO Y VISUALIZACION DE CARACTERISTICAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(X, how=\"minmax\"):\n",
    "    if how == \"minmax\":\n",
    "        scaler = MinMaxScaler()\n",
    "    elif how == \"standard\":\n",
    "        scaler = StandardScaler()\n",
    "    return scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_tsne(X, n_components=2, pca=False):\n",
    "    if pca:\n",
    "        reduccion = TSNE(n_components, init='pca')\n",
    "    else:\n",
    "        reduccion = TSNE(n_components)\n",
    "    x_new = reduccion.fit_transform(X)\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_TSNE(x,y): #plot with TSNE and PCA to reduce dimensions\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "    \n",
    "    x_new = apply_tsne(x, n_components=2, pca=True)\n",
    "    tmp_df = pd.DataFrame(np.column_stack([x_new, y]))\n",
    "    tmp_df.columns = [\"x1\", \"x2\", \"Y\"]\n",
    "    \n",
    "    sns.scatterplot(x=\"x1\", y=\"x2\", hue=\"Y\", data=tmp_df, ax=ax)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate test and train data\n",
    "def split_data(X, y, test_size=0.3):\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVEMOS COLUMNAS QUE NO QUEREMOS Y APLICAMOS SMOTE DE SER NECESARIO\n",
    "remove_columns = [\"subject\",\"day\", \"label\"] #remover columnas que no queremos para las variables\n",
    "use_smote = True\n",
    "\n",
    "X, Y = df.drop(columns=remove_columns), df[\"label\"]\n",
    "\n",
    "if use_smote:\n",
    "    smote = SMOTE(sampling_strategy='minority')\n",
    "    X, Y = smote.fit_resample(df.drop(columns=remove_columns), df[\"label\"])\n",
    "#separamos en X variables y Y labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NORMALIZAMOS LOS DATOS\n",
    "X = normalize_data(X, how=\"minmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEPARAMOS LOS DATOS DE ENTRENAMIENTO Y PRUEBA\n",
    "X_train, X_test, y_train, y_test = split_data(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOTTING DATA\n",
    "plot_data_TSNE(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENTRENAMIENTO Y TUNNING DE MODELOS PARA CLASIFICACION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(model_name=\"DecisionTree\"): \n",
    "    if model_name == \"DecisionTree\":\n",
    "        return DecisionTreeClassifier()\n",
    "    elif model_name == \"SVM\":\n",
    "        return SVC()\n",
    "    elif model_name == \"RandomForest\":\n",
    "        return RandomForestClassifier()\n",
    "    elif model_name == \"XGBoost\":\n",
    "        return XGBClassifier()\n",
    "    elif model_name == \"MLP\":\n",
    "        return MLPClassifier()\n",
    "    else:\n",
    "        print(\"Invalid model name\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparing each model accuracy\n",
    "def compare_models_accuracy(X_train, y_train, X_test, y_test):\n",
    "    models = [\"DecisionTree\", \"SVM\", \"RandomForest\"]\n",
    "    accuracies = []\n",
    "    for model_name in models:\n",
    "        model = select_model(model_name)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "    return list(zip(models, accuracies))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models_accuracy(X_train, y_train, X_test, y_test):\n",
    "    models = [\"DecisionTree\", \"SVM\", \"RandomForest\", \"XGBoost\"]\n",
    "    accuracies = []\n",
    "    for model_name in models:\n",
    "        model = select_model(model_name)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "    return list(zip(models, accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting comparison of models with no hyperparameter tuning\n",
    "comparison_list = compare_models_accuracy(X_train, y_train, X_test, y_test)\n",
    "print(comparison_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_tunning(model,parameters, method=\"grid\", scoring_metric = \"accuracy\"):\n",
    "    if method == \"grid\":\n",
    "        clf = GridSearchCV(model, param_grid=parameters, scoring=scoring_metric, cv=5, n_jobs=-1)  \n",
    "    # clf_svc = RandomizedSearchCV(clf_svc, param_distributions=parameters, scoring=\"accuracy\", cv=5, n_jobs=-1, n_iter=15)\n",
    "    else:\n",
    "        clf = RandomizedSearchCV(model, param_distributions=parameters, scoring=scoring_metric, cv=5, n_jobs=-1, n_iter=15)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"-----------------\")\n",
    "    print(\"Model\", model.__class__.__name__)\n",
    "    print(\"Best score : \", clf.best_score_)\n",
    "    print(\"Best Parameters : \", clf.best_params_)\n",
    "    print(\"-----------------\")\n",
    "    return clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Model\", model.__class__.__name__)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"F1 Score: \", f1)\n",
    "    print(\"Confusion Matrix: \")\n",
    "    print(confusion)\n",
    "    return accuracy, f1, confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = {\n",
    "#    \"n_estimators\":[i for i in range(1,200)],                   \n",
    "#    \"learning_rate\":[0.1*i for i in range(1,10)],\n",
    "#    \"max_depth\":[i for i in range(1,20)],                                            \n",
    "# }\n",
    "# useGridSearch = False\n",
    "# parameter_tunning(select_model(\"XGBoost\"), parameters, useGridSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DecisionTree parameters\n",
    "weights = np.linspace(0.0,0.99,20)\n",
    "# parameters = {\n",
    "#     \"criterion\": [\"gini\", \"entropy\",\"log_loss\"],\n",
    "#     \"max_depth\": [i if i!=0 else None for i in range(1, 10)],\n",
    "#     \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "#     'class_weight': [{0:x, 1:1.0-x , 2:1.0 - x} for x in weights]\n",
    "# }\n",
    "# model = DecisionTreeClassifier()\n",
    "# parameter_tunning(model, parameters, scoring_metric=\"f1_weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision_tree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=9, max_features=\"sqrt\")\n",
    "# decision_tree.fit(X_train, y_train)\n",
    "# y_pred = decision_tree.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy DecisionTree: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC()\n",
    "#weights = np.linspace(0.0,0.99,200)\n",
    "parameters = {\n",
    "    \"C\": [0.1, 1, 10, 100],\n",
    "    \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"], \n",
    "    \"gamma\": [0.01*i for i in range(1, 100)],\n",
    "    #'class_weight': [{0:x, 1:1.0-x,2:1.0 - x} for x in weights]\n",
    "}\n",
    "parameter_tunning(svm, parameters,scoring_metric=\"f1_weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(C=10, degree=2, gamma=0.44, kernel=\"rbf\")\n",
    "clf = svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy SVM: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    \"\"\"Create a mesh of points to plot in\"\"\"\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\"\"\"\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#APPLYTING TSNE TO VISUALIZE DATA WITH SVC\n",
    "plot_data_TSNE(X, Y)\n",
    "plt.show()\n",
    "X_tsne = apply_tsne(X, n_components=2, pca=True)\n",
    "X_train_tsne, X_test_tsne, y_train_tsne, y_test_tsne = split_data(X_tsne, Y)\n",
    "clf = svm.fit(X_train_tsne, y_train_tsne)\n",
    "\n",
    " # Create a meshgrid for plotting\n",
    "X0, X1 = X_tsne[:, 0], X_tsne[:, 1]\n",
    "xx, yy = make_meshgrid(X0, X1)\n",
    "\n",
    "# Plot the decision boundaries\n",
    "fig, ax = plt.subplots()\n",
    "plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "ax.scatter(X0, X1, c=Y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "ax.set_xlabel('TSNE Component 1')\n",
    "ax.set_ylabel('TSNE Component 2')\n",
    "ax.set_title('SVM Decision Boundary')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_f= RandomForestClassifier()\n",
    "\n",
    "# parameters = {\n",
    "#     \"n_estimators\": [50, 100, 200, 300],\n",
    "#     \"criterion\": [\"gini\", \"entropy\",\"log_loss\"],\n",
    "#     \"max_depth\": [i if i!=0 else None for i in range(1, 10)],\n",
    "#     \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "#     'class_weight': [{0:x, 1:1.0-x , 2:1.0 - x} for x in weights]\n",
    "# }\n",
    "# parameter_tunning(random_f, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_f= RandomForestClassifier(criterion=\"entropy\", max_depth=5, max_features=\"log2\", n_estimators=50)\n",
    "# random_f.fit(X_train, y_train)\n",
    "# y_pred = random_f.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy RandomForest: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost = XGBClassifier()\n",
    "# parameters = {\n",
    "#    \"n_estimators\":[i for i in range(1,200)],                   \n",
    "#    \"learning_rate\":[0.1*i for i in range(1,10)],\n",
    "#    \"max_depth\":[i for i in range(1,20)],\n",
    "#    \"max_features\":[i for i in range(1,20)],\n",
    "#    'class_weight': [{0:x, 1:1.0-x , 2:1.0 - x} for x in weights]\n",
    "                                                  \n",
    "# }\n",
    "# parameter_tunning(xgboost, parameters, method=\"random\", scoring_metric=\"f1_weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp_model = MLPClassifier()\n",
    "# parameters = {\n",
    "#     \"hidden_layer_sizes\": [(100,), (200,), (300,)],\n",
    "#     \"activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
    "#     \"solver\": [\"lbfgs\", \"sgd\", \"adam\"],\n",
    "#     \"alpha\": [0.0001, 0.001, 0.01, 0.1],\n",
    "#     \"learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"]\n",
    "# }\n",
    "# parameter_tunning(mlp_model, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp_model = MLPClassifier(activation=\"relu\", alpha=0.1, hidden_layer_sizes=(100,), learning_rate=\"invscaling\", solver=\"adam\")\n",
    "# mlp_model.fit(X_train, y_train)\n",
    "# y_pred = mlp_model.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy MLP: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decimal_to_time(decimal_time): ##calculo para pasar tiempo decimal a horas minutos y segundos\n",
    "    \"\"\"\n",
    "    Convert a decimal time representation to HH:MM:SS format.\n",
    "    \n",
    "    Parameters:\n",
    "    decimal_time (float): The decimal representation of time.\n",
    "    \n",
    "    Returns:\n",
    "    str: The time in HH:MM:SS format.\n",
    "    \"\"\"\n",
    "    total_seconds = int(decimal_time * 24 * 3600)\n",
    "    hours = total_seconds // 3600\n",
    "    minutes = (total_seconds % 3600) // 60\n",
    "    seconds = total_seconds % 60\n",
    "    return f\"{hours:02}:{minutes:02}:{seconds:02}\"\n",
    "\n",
    "\n",
    "print(decimal_to_time(0.457638889)) \n",
    "print(decimal_to_time(0.707638889)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANALISIS Y COMPARACION DE MODELOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_precision_recall_curve(y_test, y_score, n_classes=3):\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    for i in range(n_classes):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_test[:, i],\n",
    "                                                            y_score[:, i])\n",
    "        plt.plot(recall[i], precision[i], lw=2, label='class {}'.format(i))\n",
    "        \n",
    "    plt.xlabel(\"recall\")\n",
    "    plt.ylabel(\"precision\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"precision vs. recall curve\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifiers with best hyperparameters\n",
    "#SMOTE\n",
    "decision_tree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=5, max_features=\"log2\")\n",
    "xgbModel = XGBClassifier(learning_rate=0.2, max_depth=19, n_estimators=154)\n",
    "svm = SVC(C=100, degree=2, gamma=0.09, kernel=\"rbf\")\n",
    "random_f= RandomForestClassifier(criterion=\"entropy\", max_depth=6, max_features=\"sqrt\", n_estimators=50)\n",
    "mlp_model = MLPClassifier(activation=\"relu\", alpha=0.01, hidden_layer_sizes=(200,), learning_rate=\"invscaling\", solver=\"adam\")\n",
    "\n",
    "#WEIGHTED\n",
    "# decision_tree = DecisionTreeClassifier(criterion=\"log_loss\", max_depth=6, max_features=\"sqrt\",class_weight={0: np.float64(0.5074371859296483), 1: np.float64(0.49256281407035174), 2: np.float64(0.49256281407035174)})\n",
    "# xgbModel = XGBClassifier(learning_rate=0.4, max_depth=15, n_estimators=10,class_weight={0: np.float64(0.8336842105263158), 1: np.float64(0.1663157894736842), 2: np.float64(0.1663157894736842)})\n",
    "# svm = SVC(C=100, gamma=0.03, kernel=\"rbf\",probability=True,class_weight={0: 0.5223618090452261, 1:0.4776381909547739, 2:0.4776381909547739}) \n",
    "# random_f= RandomForestClassifier(criterion=\"log_loss\", max_depth=9, max_features=\"log2\", n_estimators=50 , class_weight= {0: np.float64(0.10421052631578948), 1: np.float64(0.8957894736842105), 2: np.float64(0.8957894736842105)} )\n",
    "# mlp_model = MLPClassifier(activation=\"relu\", alpha=0.01, hidden_layer_sizes=(200,), learning_rate=\"invscaling\", solver=\"adam\")\n",
    "train_and_evaluate_model(decision_tree, X_train, y_train, X_test, y_test)\n",
    "train_and_evaluate_model(svm, X_train, y_train, X_test, y_test)\n",
    "train_and_evaluate_model(random_f, X_train, y_train, X_test, y_test)\n",
    "train_and_evaluate_model(xgbModel, X_train, y_train, X_test, y_test)\n",
    "train_and_evaluate_model(mlp_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WEIGHING CLASSES\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "svm = SVC(probability=True)\n",
    "\n",
    "weights = np.linspace(0.0,0.99,200)\n",
    "param_grid = {'class_weight': [{0:x, 1:1.0-x} for x in weights]}\n",
    "#Fitting grid search to the train data with 5 folds\n",
    "gridsearch_csv_results = parameter_tunning(svm, param_grid, method=\"grid\")\n",
    "\n",
    "#Ploting the score for different values of weight\n",
    "sns.set_style('whitegrid')\n",
    "plt.figure(figsize=(12,8))\n",
    "weigh_data = pd.DataFrame({ 'score': gridsearch_csv_results['mean_test_score'], 'weight': (1- weights)})\n",
    "sns.lineplot(data=weigh_data, y='score', x='weight')\n",
    "plt.xlabel('Weight for class 1')\n",
    "plt.ylabel('F1 score')\n",
    "plt.xticks([round(i/10,1) for i in range(0,11,1)])\n",
    "plt.title('Scoring for different class weights', fontsize=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing model with best hyperparameters and weights or smote\n",
    "#model = SVC(C=100, gamma=0.03, kernel=\"rbf\",probability=True,class_weight={0: 0.5223618090452261, 1:0.4776381909547739, 2:0.4776381909547739}) #model without smote and applying weights\n",
    "model = SVC(C=100, degree=2, gamma=0.09, kernel=\"rbf\",probability=True) #model with smote\n",
    "train_and_evaluate_model(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "n_classes = 3\n",
    "Y_pr = label_binarize(Y, classes=[0, 1, 2])#esto es necesario para la curva de precision recall\n",
    "X_train_pr, X_test_pr, y_train_pr, y_test_pr = train_test_split(X,\n",
    "                                                    Y_pr,\n",
    "                                                    random_state = 42)\n",
    "clf = OneVsRestClassifier(model)\n",
    "clf.fit(X_train_pr, y_train_pr)\n",
    "y_score = clf.predict_proba(X_test_pr)\n",
    "\n",
    "graph_precision_recall_curve(y_test_pr, y_score, n_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REGRESION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Preparar los datos\n",
    "new_df_regression = df.drop(['subject','label'], axis=1)\n",
    "new_X = new_df_regression.drop('calories burnt (kcal)', axis=1)\n",
    "new_Y = new_df_regression['calories burnt (kcal)']\n",
    "\n",
    "# Filas con valores nulos\n",
    "new_X = new_X.dropna()\n",
    "new_Y = new_Y[new_X.index]\n",
    "\n",
    "# Calcular la correlación\n",
    "correlation_matrix = new_df_regression.corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Matriz de Correlación')\n",
    "plt.show()\n",
    "\n",
    "# Seleccionar las variables más correlacionadas con la variable objetivo\n",
    "correlation_threshold = 0.5\n",
    "correlated_features = correlation_matrix.index[abs(correlation_matrix['calories burnt (kcal)']) > correlation_threshold].tolist()\n",
    "correlated_features.remove('calories burnt (kcal)')\n",
    "new_X = new_X[correlated_features]\n",
    "\n",
    "# Separar en datos de entrenamiento y prueba\n",
    "# new_X_train, new_X_test, new_Y_train, new_Y_test = split_data(new_X, new_Y)\n",
    "\n",
    "# # Normalizamos los datos\n",
    "# new_X_train = normalize_data(new_X_train, how=\"minmax\")\n",
    "# new_X_test = normalize_data(new_X_test, how=\"minmax\")\n",
    "\n",
    "# Normalizar los datos\n",
    "scaler = StandardScaler()\n",
    "new_X = scaler.fit_transform(new_X)\n",
    "\n",
    "# Entrenar el modelo\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(max_depth=5),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=50, max_depth=5),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3),\n",
    "    \"Support Vector Regression\": SVR(C=100, gamma=0.01)\n",
    "}\n",
    "\n",
    "# Evaluar cada modelo con validación cruzada\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, new_X, new_Y, cv=5, scoring='neg_mean_squared_error')\n",
    "    mse = -scores.mean()\n",
    "    mae = -cross_val_score(model, new_X, new_Y, cv=5, scoring='neg_mean_absolute_error').mean()\n",
    "    r2 = cross_val_score(model, new_X, new_Y, cv=5, scoring='r2').mean()\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Mean Squared Error: {mse}\")\n",
    "    print(f\"  Mean Absolute Error: {mae}\")\n",
    "    print(f\"  R² Score: {r2}\")\n",
    "    print(f\"  Root Mean Squared Error: {rmse}\")\n",
    "    print()\n",
    "    \n",
    "    # Ajustar el modelo y predecir\n",
    "    model.fit(new_X, new_Y)\n",
    "    new_Y_pred = model.predict(new_X)\n",
    "    \n",
    "    # Graficar resultados\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Gráfico de dispersión de valores reales vs. predichos\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(new_Y, new_Y_pred, alpha=0.5)\n",
    "    plt.plot([new_Y.min(), new_Y.max()], [new_Y.min(), new_Y.max()], 'k--', lw=2)\n",
    "    plt.xlabel('Valores Reales')\n",
    "    plt.ylabel('Valores Predichos')\n",
    "    plt.title(f'{name} - Valores Reales vs. Predichos')\n",
    "    \n",
    "    # Gráfico de residuos\n",
    "    plt.subplot(1, 2, 2)\n",
    "    residuals = new_Y - new_Y_pred\n",
    "    plt.scatter(new_Y_pred, residuals, alpha=0.5)\n",
    "    plt.hlines(y=0, xmin=new_Y_pred.min(), xmax=new_Y_pred.max(), colors='r', linestyles='dashed')\n",
    "    plt.xlabel('Valores Predichos')\n",
    "    plt.ylabel('Residuos')\n",
    "    plt.title(f'{name} - Gráfico de Residuos')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
